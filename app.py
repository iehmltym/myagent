from collections import OrderedDict
import re
import time
from typing import Any, Dict, List, Optional, Tuple

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse, StreamingResponse
from pydantic import BaseModel

from knowledge_base import blueprint_payload, retrieve_documents
from my_custom_llm import MyCustomGeminiLLM
from tools import add, get_date, get_time, make_uuid, safe_eval_expression, text_stats

app = FastAPI()
# ä¼šè¯å†å²å­—å…¸ï¼škey æ˜¯ session_idï¼Œvalue æ˜¯ [(question, answer), ...]
# ç”¨äºåœ¨å¤šè½®å¯¹è¯ä¸­ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œä¾¿äºæ‹¼æ¥æˆè¿ç»­å¯¹è¯çš„ prompt
sessions: "OrderedDict[str, List[Tuple[str, str]]]" = OrderedDict()
MAX_HISTORY_TURNS = 10
MAX_SESSIONS = 200

answer_cache: "OrderedDict[Tuple[str, str], str]" = OrderedDict()
MAX_CACHE_ITEMS = 256
cache_stats = {"hits": 0, "misses": 0}
request_stats = {"total": 0, "llm": 0, "rule": 0, "cache": 0}

COMMAND_PATTERN = re.compile(r"^/([a-zA-Z]+)\s*(.*)$")
MATH_PATTERN = re.compile(r"\s*(\d+(?:\.\d+)?)\s*\+\s*(\d+(?:\.\d+)?)\s*=\s*\?\s*")

# å…è®¸ä½ çš„ GitHub Pages è°ƒç”¨ï¼ˆ/ai ä»å±äºåŒä¸€åŸŸåï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://iehmltym.github.io"],
    allow_origin_regex=r"https://iehmltym\.github\.io$",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
    max_age=86400,
)

# é»˜è®¤çš„å¯çˆ±è¯­æ°” system promptï¼ˆå½“ç”¨æˆ·æœªæä¾›æ—¶ä½¿ç”¨ï¼‰
DEFAULT_CUTE_SYSTEM_PROMPT = "è¯·ç”¨å¯çˆ±çš„è¯­æ°”å›ç­”ï¼Œç®€æ´ã€æ¸©æŸ”ï¼Œåƒå°å¯çˆ±ä¸€æ ·ï½"
ADVISOR_SYSTEM_PROMPT = (
    "ä½ æ˜¯èµ„æ·± AI æ¶æ„é¡¾é—®ï¼Œæ“…é•¿ LangChain/Agent/RAG é¡¹ç›®è½åœ°ã€‚"
    "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“æ„å¿…é¡»åŒ…å«ï¼š"
    "1) æ ¸å¿ƒèƒ½åŠ›ï¼ˆLangChain èƒ½åšä»€ä¹ˆï¼‰"
    "2) å¾ˆå¼ºçš„å…¸å‹ç”¨æ³•ï¼ˆè½åˆ°çœŸå®é¡¹ç›®ï¼‰"
    "3) Render 512MB å…è´¹å®ä¾‹çš„è½åœ°å»ºè®®"
    "4) é¡¹ç›®ä¸‹ä¸€æ­¥é«˜ä»·å€¼å‡çº§"
    "ç»™å‡ºæ¸…æ™°çš„å°æ ‡é¢˜å’Œè¦ç‚¹åˆ—è¡¨ï¼Œè¯­æ°”ä¸“ä¸šã€ç›´æ¥ã€‚"
)
ROADMAP_FEATURES = [
    {
        "title": "ğŸ“ æ‹–æ‹½æ–‡ä»¶ä¸Šä¼ ï¼ˆæ–‡æ¡£é—®ç­”ï¼‰",
        "desc": "å‰ç«¯æ”¯æŒæ‹–æ‹½ PDF/Markdown/TXTï¼Œåç«¯åªåšè½»é‡è§£æä¸ä¸Šä¼ å­˜å‚¨ã€‚",
        "impact": "ä¸Šä¼ è§£æä¼šå ç”¨å†…å­˜ï¼Œå»ºè®®ç¦»çº¿å¤„ç†æˆ–å¼‚æ­¥é˜Ÿåˆ—ã€‚",
    },
    {
        "title": "ğŸ™ï¸ è¯­éŸ³è¾“å…¥ / è¯­éŸ³è¾“å‡º",
        "desc": "æµè§ˆå™¨ç«¯å½•éŸ³ â†’ è¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰â†’ LLM å›å¤ â†’ è¯­éŸ³åˆæˆï¼ˆTTSï¼‰ã€‚",
        "impact": "å»ºè®®å‰ç«¯è°ƒç”¨ç¬¬ä¸‰æ–¹ STT/TTSï¼ŒæœåŠ¡å™¨åªè½¬å‘ç»“æœã€‚",
    },
    {
        "title": "ğŸ§  RAG çŸ¥è¯†åº“æ›´æ–°å…¥å£",
        "desc": "æä¾›ä¸Šä¼ /åŒæ­¥å…¥å£ï¼Œè§¦å‘ç¦»çº¿ embedding æ›´æ–°ç´¢å¼•ã€‚",
        "impact": "512MB å®ä¾‹åªè´Ÿè´£æ£€ç´¢ï¼Œå‘é‡åº“æ”¾å¤–éƒ¨æ‰˜ç®¡ã€‚",
    },
    {
        "title": "ğŸ§­ æ™ºèƒ½ Router",
        "desc": "ç®€å•é—®é¢˜èµ°è½»æ¨¡å‹ï¼Œå¤æ‚é—®é¢˜èµ°å¼ºæ¨¡å‹ + æ£€ç´¢é“¾ã€‚",
        "impact": "å¯æ˜¾è‘—é™æˆæœ¬ï¼Œé€‚åˆå…è´¹å®ä¾‹ã€‚",
    },
    {
        "title": "ğŸ§© è½»é‡ Agent å·¥å…·ç®±",
        "desc": "é™å®šå°‘é‡é«˜ç¡®å®šæ€§å·¥å…·ï¼ˆæœç´¢/è®¡ç®—/DB æŸ¥è¯¢ï¼‰ï¼Œé˜²æ­¢å‘æ•£ã€‚",
        "impact": "é™åˆ¶å·¥å…·æ•°é‡ï¼Œå‡å°‘ token ä¸å»¶è¿Ÿã€‚",
    },
    {
        "title": "ğŸ“Š è½»é‡è§‚æµ‹ + å¯¼å‡º",
        "desc": "æ”¯æŒå¯¼å‡ºè¯·æ±‚æ—¥å¿—/å‘½ä¸­æƒ…å†µï¼Œä¾¿äºæ’æŸ¥å›ç­”è´¨é‡ã€‚",
        "impact": "æ—¥å¿—è½ç›˜ + è½®è½¬ï¼Œé¿å…å ç”¨å†…å­˜ã€‚",
    },
]


class QuestionRequest(BaseModel):
    # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜æ–‡æœ¬
    question: str
    # å¯é€‰çš„ system promptï¼Œç”¨æ¥å½±å“æ¨¡å‹è¾“å‡ºé£æ ¼
    # å¦‚æœä¸ä¼ æˆ–ä¼ ç©ºå­—ç¬¦ä¸²ï¼Œå°±ä¼šèµ°é»˜è®¤çš„ MyGeminiLLM
    system_prompt: Optional[str] = None
    # ä¼šè¯ IDï¼šç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·/å¯¹è¯çš„ä¸Šä¸‹æ–‡
    # ä¸ºç©ºæ—¶è¡¨ç¤ºå•è½®è¯·æ±‚ï¼Œä¸è®°å½•å†å²
    session_id: Optional[str] = None
    # æ˜¯å¦å¯ç”¨æµå¼å“åº”ï¼ˆé»˜è®¤ True å…¼å®¹æ—§ç‰ˆå‰ç«¯ï¼‰
    stream: bool = True
    # æ˜¯å¦è¿”å›å…ƒä¿¡æ¯ï¼ˆå¦‚ç¼“å­˜å‘½ä¸­ã€è€—æ—¶ç­‰ï¼‰
    include_meta: bool = False
    # è¿è¡Œæ¨¡å¼ï¼šé»˜è®¤ç©ºï¼›"advisor" è¡¨ç¤ºæ¶æ„å»ºè®®ï¼›"rag" è¡¨ç¤ºæ£€ç´¢å¢å¼º
    mode: Optional[str] = None


class SessionRequest(BaseModel):
    session_id: str


@app.get("/health")
def health():
    return {"ok": True}


@app.get("/", response_class=HTMLResponse)
def index():
    template_path = "templates/index.html"
    try:
        return FileResponse(template_path)
    except FileNotFoundError:
        return """
        <!doctype html>
        <html lang="zh">
        <head><meta charset="utf-8"><title>MyAgent API</title></head>
        <body>
          <h2>MyAgent API is running</h2>
          <p>Health: <a href="/health">/health</a></p>
          <p>Use <code>POST /ask</code> with JSON: {"question":"..."}</p>
        </body>
        </html>
        """


@app.get("/features")
def features() -> Dict[str, Any]:
    return {
        "commands": [
            {"cmd": "/time", "desc": "æŸ¥çœ‹å½“å‰æ—¶é—´"},
            {"cmd": "/date", "desc": "æŸ¥çœ‹ä»Šæ—¥æ—¥æœŸ"},
            {"cmd": "/uuid", "desc": "ç”Ÿæˆ UUID4"},
            {"cmd": "/calc 1+2*3", "desc": "å®‰å…¨è®¡ç®—è¡¨è¾¾å¼"},
            {"cmd": "/len æ–‡æœ¬", "desc": "æŸ¥çœ‹æ–‡æœ¬ç»Ÿè®¡"},
            {"cmd": "/help", "desc": "æŸ¥çœ‹å¯ç”¨å‘½ä»¤"},
        ],
        "limits": {
            "max_history_turns": MAX_HISTORY_TURNS,
            "max_sessions": MAX_SESSIONS,
            "cache_size": MAX_CACHE_ITEMS,
        },
        "modes": [
            {"id": "advisor", "desc": "æ¶æ„å»ºè®®æ¨¡å¼ï¼šæŒ‰å›ºå®šç»“æ„è¾“å‡ºå‡çº§å»ºè®®"},
            {"id": "rag", "desc": "æ£€ç´¢å¢å¼ºæ¨¡å¼ï¼šç»“åˆå†…ç½®èµ„æ–™è¾“å‡º"},
        ],
    }


@app.get("/stats")
def stats() -> Dict[str, Any]:
    return {
        "requests": request_stats,
        "cache": {"size": len(answer_cache), **cache_stats},
        "sessions": {"active": len(sessions), "max": MAX_SESSIONS},
    }


@app.get("/blueprint")
def blueprint() -> Dict[str, Any]:
    return blueprint_payload()


@app.get("/roadmap")
def roadmap() -> Dict[str, Any]:
    return {"features": ROADMAP_FEATURES}


@app.post("/session/clear")
def clear_session(req: SessionRequest) -> Dict[str, Any]:
    if sessions.pop(req.session_id, None) is not None:
        return {"cleared": True, "session_id": req.session_id}
    return {"cleared": False, "session_id": req.session_id}


def _get_session_history(session_key: Optional[str]) -> List[Tuple[str, str]]:
    if not session_key:
        return []
    history = sessions.get(session_key)
    if history is None:
        history = []
        sessions[session_key] = history
    sessions.move_to_end(session_key)
    if len(sessions) > MAX_SESSIONS:
        sessions.popitem(last=False)
    return history


def _cache_get(key: Tuple[str, str]) -> Optional[str]:
    if key in answer_cache:
        answer_cache.move_to_end(key)
        cache_stats["hits"] += 1
        request_stats["cache"] += 1
        return answer_cache[key]
    cache_stats["misses"] += 1
    return None


def _cache_set(key: Tuple[str, str], answer: str) -> None:
    answer_cache[key] = answer
    answer_cache.move_to_end(key)
    if len(answer_cache) > MAX_CACHE_ITEMS:
        answer_cache.popitem(last=False)


def _handle_command(question: str) -> Optional[Tuple[str, Dict[str, Any]]]:
    match = COMMAND_PATTERN.match(question.strip())
    if not match:
        return None
    cmd = match.group(1).lower()
    payload = match.group(2).strip()
    if cmd == "time":
        return get_time(), {"command": "/time"}
    if cmd == "date":
        return get_date(), {"command": "/date"}
    if cmd == "uuid":
        return make_uuid(), {"command": "/uuid"}
    if cmd == "calc":
        if not payload:
            return "è¯·è¾“å…¥è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ /calc 1+2*3", {"command": "/calc"}
        try:
            return str(safe_eval_expression(payload)), {"command": "/calc"}
        except ValueError as exc:
            return f"è®¡ç®—å¤±è´¥ï¼š{exc}", {"command": "/calc", "error": True}
    if cmd == "len":
        if not payload:
            return "è¯·è¾“å…¥è¦ç»Ÿè®¡çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ /len hello world", {"command": "/len"}
        stats = text_stats(payload)
        return (
            f"å­—ç¬¦æ•°ï¼š{stats['chars']} | å»ç©ºæ ¼å­—ç¬¦æ•°ï¼š{stats['chars_no_space']} | å•è¯æ•°ï¼š{stats['words']}",
            {"command": "/len"},
        )
    if cmd == "help":
        return (
            "å¯ç”¨å‘½ä»¤ï¼š/time /date /uuid /calc è¡¨è¾¾å¼ /len æ–‡æœ¬ /help",
            {"command": "/help"},
        )
    return "æœªçŸ¥å‘½ä»¤ï¼Œè¯•è¯• /help", {"command": f"/{cmd}", "error": True}


@app.post("/ask")
def ask_question(req: QuestionRequest):
    # å–å‡ºç”¨æˆ·è¾“å…¥çš„é—®é¢˜æ–‡æœ¬ï¼Œä¾¿äºåç»­å¤„ç†
    question = req.question.strip()
    stream = req.stream
    include_meta = req.include_meta
    forced_stream_off = False
    if include_meta and stream:
        stream = False
        forced_stream_off = True

    started = time.monotonic()
    request_stats["total"] += 1

    def respond_text(answer: str, *, meta: Optional[Dict[str, Any]] = None):
        if stream:
            def answer_stream():
                yield answer

            headers = {}
            if meta:
                headers["X-Answer-Source"] = meta.get("source", "")
                headers["X-Cache-Hit"] = str(meta.get("cache_hit", False)).lower()
            return StreamingResponse(answer_stream(), media_type="text/plain; charset=utf-8", headers=headers)
        payload = {"answer": answer}
        if include_meta and meta:
            payload["meta"] = meta
        return JSONResponse(payload)

    def update_history(session_key: Optional[str], answer: str):
        if not session_key:
            return
        history = _get_session_history(session_key)
        history.append((question, answer))
        if len(history) > MAX_HISTORY_TURNS:
            sessions[session_key] = history[-MAX_HISTORY_TURNS:]

    def build_meta(source: str, cache_hit: bool, extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        meta = {
            "source": source,
            "cache_hit": cache_hit,
            "latency_ms": round((time.monotonic() - started) * 1000, 2),
        }
        if forced_stream_off:
            meta["stream_forced_off"] = True
        if extra:
            meta.update(extra)
        return meta

    command_result = _handle_command(question)
    if command_result:
        answer, extra = command_result
        request_stats["rule"] += 1
        return respond_text(answer, meta=build_meta("command", False, extra))
    # è§„åˆ™ 1ï¼šå¦‚æœé—®é¢˜ä¸­å‡ºç°â€œç°åœ¨å‡ ç‚¹â€æˆ–â€œä»Šå¤©æ—¥æœŸâ€ï¼Œç›´æ¥è¿”å›å½“å‰æ—¶é—´
    if "ç°åœ¨å‡ ç‚¹" in question or "ä»Šå¤©æ—¥æœŸ" in question:
        request_stats["rule"] += 1
        return respond_text(get_time(), meta=build_meta("rule", False, {"tool": "time"}))

    # è§„åˆ™ 2ï¼šå¦‚æœé—®é¢˜ç¬¦åˆ â€œa+b=?â€ å½¢å¼ï¼Œè§£æå‡º aã€b å¹¶è®¡ç®—
    # è¯´æ˜ï¼šä¸‹é¢è¿™ä¸ªæ­£åˆ™å…è®¸ç©ºæ ¼å’Œå°æ•°ï¼Œæ¯”å¦‚ " 1 + 2 = ? "
    math_match = MATH_PATTERN.fullmatch(question)
    if math_match:
        # æ­£åˆ™åˆ†ç»„ 1 å’Œ 2 åˆ†åˆ«æ˜¯ aã€b çš„æ–‡æœ¬å½¢å¼
        left = float(math_match.group(1))
        right = float(math_match.group(2))
        # è®¡ç®—å®Œæˆåç›´æ¥è¿”å›ï¼Œé¿å…èµ° LLM
        request_stats["rule"] += 1
        return respond_text(str(add(left, right)), meta=build_meta("rule", False, {"tool": "add"}))

    # å¦‚æœå‰ç«¯ä¼ äº† system_promptï¼Œå°±ç”¨å®ƒï¼›
    # å¦åˆ™ä½¿ç”¨é»˜è®¤çš„å¯çˆ±è¯­æ°” promptã€‚
    system_prompt = req.system_prompt or DEFAULT_CUTE_SYSTEM_PROMPT
    mode = (req.mode or "").strip().lower()
    rag_context = ""
    rag_meta: Dict[str, Any] = {}
    if mode in {"advisor", "rag"}:
        if not req.system_prompt:
            system_prompt = ADVISOR_SYSTEM_PROMPT
        rag_docs = retrieve_documents(question, top_k=4)
        if rag_docs:
            rag_context = "\n\n".join(
                f"[{index + 1}] {doc['title']}\n{doc['content']}" for index, doc in enumerate(rag_docs)
            )
            rag_meta = {
                "rag_docs": [doc["title"] for doc in rag_docs],
                "rag_count": len(rag_docs),
            }
    # å§‹ç»ˆç”¨è‡ªå®šä¹‰ LLM åŒ…è£…å™¨æ³¨å…¥ system promptï¼Œè®©å›ç­”ä¿æŒå¯çˆ±é£æ ¼
    active_llm = MyCustomGeminiLLM(prefix=system_prompt)
    # è§„èŒƒåŒ– session_idï¼ˆå»æ‰é¦–å°¾ç©ºç™½ï¼‰ï¼Œé¿å…åŒä¸€ä¼šè¯è¢«å½“æˆå¤šä¸ª key
    session_id = req.session_id.strip() if req.session_id else None
    # è·å–è¯¥ä¼šè¯çš„å†å²ï¼›è‹¥ä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
    # æœªæä¾› session_id æ—¶è§†ä¸ºå•è½®å¯¹è¯ï¼Œä¸è¯»å–/å†™å…¥å†å²
    history = _get_session_history(session_id) if session_id else []
    # å°†å†å²è®°å½•æ‹¼æˆè¿ç»­å¯¹è¯çš„ promptï¼Œæ ¼å¼å¦‚ï¼š
    # ç”¨æˆ·ï¼š... \n åŠ©æ‰‹ï¼š... \n ç”¨æˆ·ï¼š... \n åŠ©æ‰‹ï¼š...
    history_prompt = "".join(
        f"ç”¨æˆ·ï¼š{user_question}\nåŠ©æ‰‹ï¼š{response}\n" for user_question, response in history
    )
    # æ‹¼æ¥æœ¬æ¬¡é—®é¢˜ï¼Œæç¤ºæ¨¡å‹ç»§ç»­å›å¤åŠ©æ‰‹å†…å®¹
    if rag_context:
        prompt = (
            f"{history_prompt}ç”¨æˆ·ï¼š{req.question}\n\n"
            f"è¯·å‚è€ƒä»¥ä¸‹èµ„æ–™ä½œç­”ï¼ˆä¸è¦é€å­—å¤è¿°ï¼Œä¿æŒç»“æ„åŒ–è¾“å‡ºï¼‰ï¼š\n{rag_context}\n\n"
            "åŠ©æ‰‹ï¼š"
        )
    else:
        prompt = f"{history_prompt}ç”¨æˆ·ï¼š{req.question}\nåŠ©æ‰‹ï¼š"
    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆï¼Œmax_output_tokens é€‚å½“æé«˜ä»¥é¿å…å›ç­”è¢«æˆªæ–­
    cache_key = (system_prompt, question)
    if not session_id:
        cached_answer = _cache_get(cache_key)
        if cached_answer is not None:
            return respond_text(cached_answer, meta=build_meta("cache", True, rag_meta))

    if not stream:
        answer = active_llm.generate(prompt, max_output_tokens=2048)
        request_stats["llm"] += 1
        if not session_id:
            _cache_set(cache_key, answer)
        update_history(session_id, answer)
        return respond_text(
            answer,
            meta=build_meta("llm", False, {"model": active_llm.model_name, **rag_meta}),
        )

    def answer_stream():
        answer_parts = []
        for chunk in active_llm.generate_stream(prompt, max_output_tokens=2048):
            answer_parts.append(chunk)
            yield chunk
        final_answer = "".join(answer_parts)
        request_stats["llm"] += 1
        if not session_id:
            _cache_set(cache_key, final_answer)
        update_history(session_id, final_answer)

    return StreamingResponse(answer_stream(), media_type="text/plain; charset=utf-8")
